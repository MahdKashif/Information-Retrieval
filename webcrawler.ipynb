{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmDfx6VtHCGW"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GP--NYuOHCGu"
      },
      "outputs": [],
      "source": [
        "visited_urls = []\n",
        "to_visit_urls = [\"http://www.mit.edu\"]\n",
        "num_pages_visited = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1mPh1MfHCGy"
      },
      "outputs": [],
      "source": [
        "# Function to check if URL is allowed by robots.txt\n",
        "def is_allowed_by_robots(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    robots_url = parsed_url.scheme + \"://\" + parsed_url.netloc + \"/robots.txt\"\n",
        "    try:\n",
        "        robots = requests.get(robots_url)\n",
        "        robots_text = robots.text\n",
        "    except:\n",
        "        # Assume URL is allowed if there is an error\n",
        "        return True\n",
        "    for line in robots_text.split(\"\\n\"):\n",
        "        if line.startswith(\"Disallow:\"):\n",
        "            disallowed_path = line.split(\":\")[1].strip()\n",
        "            if disallowed_path == \"/\" or parsed_url.path.startswith(disallowed_path):\n",
        "                return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLNxkNAVHCG3"
      },
      "outputs": [],
      "source": [
        "# Loop until we've visited 100 pages or there are no more pages to visit\n",
        "while num_pages_visited < 100 and len(to_visit_urls) > 0:\n",
        "    # Get next URL to visit\n",
        "    next_url = to_visit_urls.pop(0)\n",
        "\n",
        "    # Check if URL has already been visited\n",
        "    if next_url in visited_urls:\n",
        "        continue\n",
        "\n",
        "    # Check if URL is allowed by robots.txt\n",
        "    if not is_allowed_by_robots(next_url):\n",
        "        continue\n",
        "\n",
        "    # Send HTTP request and get response\n",
        "    try:\n",
        "        response = requests.get(next_url)\n",
        "    except:\n",
        "        # Skip this URL if there is an error\n",
        "        continue\n",
        "\n",
        "    # Check if content type is HTML\n",
        "    content_type = response.headers.get(\"Content-Type\", \"\")\n",
        "    if \"text/html\" not in content_type:\n",
        "        # Skip this URL if content type is not HTML\n",
        "        continue\n",
        "\n",
        "    # Get canonical URL\n",
        "    canonical_url = response.url\n",
        "\n",
        "    # Get list of outgoing links\n",
        "    outgoing_links = []\n",
        "\n",
        "    # Parse HTML content and extract links\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    for link in soup.find_all(\"a\"):\n",
        "        href = link.get(\"href\", \"\")\n",
        "        # Check if href is a valid URL\n",
        "        if not href.startswith((\"http://\", \"https://\")):\n",
        "            continue\n",
        "        # Check if href is within the mit.edu domain\n",
        "        if not urlparse(href).netloc.endswith(\"mit.edu\"):\n",
        "            continue\n",
        "        # Convert href to canonical form\n",
        "        href = urljoin(canonical_url, href)\n",
        "        href = re.sub(r\"#.*\", \"\", href)\n",
        "        if href not in outgoing_links and href != canonical_url:\n",
        "            outgoing_links.append(href)\n",
        "\n",
        "    # Add outgoing links to list of URLs to visit\n",
        "    to_visit_urls.extend(outgoing_links)\n",
        "\n",
        "    # Mark this URL as visited\n",
        "    visited_urls.append(next_url)\n",
        "\n",
        "    # Write URLs to output file\n",
        "    with open(\"output.txt\", \"a\") as f:\n",
        "        f.write(canonical_url + \" \" + \" \".join(outgoing_links) + \"\\n\")\n",
        "\n",
        "    # Increment number of pages visited\n",
        "    num_pages_visited += 1\n",
        "\n",
        "    # Wait five seconds before visiting next page\n",
        "    time.sleep(5)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}