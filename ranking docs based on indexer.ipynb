{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQPS2yV4uDmW"
      },
      "source": [
        "Important Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT3yGar0uDmh",
        "outputId": "2ae3069a-5e22-40f0-a41e-ad9554d43090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.2)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=530dcfbc44faf8b93de992f9b955451e74b22e0d833bbf806043a06f69763bfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ],
      "source": [
        "#read docx file\n",
        "!pip install python-docx\n",
        "import docx\n",
        "from docx import Document\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVwAGJxeuDmp"
      },
      "source": [
        "Functions for Getting Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uVK1HduJuDmq"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "inverted_index = {}\n",
        "\n",
        "with open('term_index.txt', 'r') as lines:\n",
        "    \n",
        "    #iterate in every line\n",
        "    for line in lines:\n",
        "\n",
        "      #store the previous doc ids\n",
        "      temp=-1\n",
        "      temp1=-1\n",
        "\n",
        "      #split the line on the basis of space\n",
        "      for index, data in enumerate(line.strip().split(' ')):\n",
        "\n",
        "        if index == 0:\n",
        "\n",
        "          #get the document id\n",
        "          id=int(data)\n",
        "\n",
        "          #for every document id we have to crate the list\n",
        "          inverted_index[id] = defaultdict(list)\n",
        "        else:\n",
        "\n",
        "\n",
        "          flag=True\n",
        "\n",
        "          #split on the basis of : and then check the length\n",
        "          if len(data.split(\":\")) == 1:\n",
        "            flag=False\n",
        "            position = int(data.split(\":\")[0])\n",
        "          else:\n",
        "\n",
        "            #get the document id\n",
        "            doument_id = int(data.split(\":\")[0])      \n",
        "\n",
        "            #get the psotions         \n",
        "            position = int(data.split(\":\")[1])\n",
        "\n",
        "          #if temp = -1 means we didnot have stored any thing yet\n",
        "          if temp == -1:\n",
        "\n",
        "            #append the positios in the dixtionary\n",
        "            inverted_index[id][doument_id].append(position)\n",
        "            temp = doument_id\n",
        "            temp1 = position\n",
        "          else:\n",
        "\n",
        "            #if the flag is false \n",
        "            if flag == False:\n",
        "              temp1 += position\n",
        "              inverted_index[id][temp].append(temp1)\n",
        "            else:\n",
        "\n",
        "              #store the document id in the temp variable for the future use\n",
        "              temp += doument_id\n",
        "\n",
        "              #store the positions in the dictionary\n",
        "              inverted_index[id][temp].append(position)      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TudypsZOuDmt"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty dictionary\n",
        "term_info_dict = {}\n",
        "\n",
        "# Open the file for reading\n",
        "with open(\"/content/term_info (2).txt\", \"r\") as file:\n",
        "    # Iterate over each line in the file\n",
        "    for line in file:\n",
        "        # Split the line using the \"/\" separator\n",
        "        line_items = line.strip().split(\"/\")\n",
        "        # Extract the key and values\n",
        "        key = int(line_items[0])\n",
        "        values = [int(value) for value in line_items[1:]]\n",
        "        # print('key', key, 'value:', values)\n",
        "        # Add the key and values to the dictionary\n",
        "        term_info_dict[key] = values\n",
        "        \n",
        "doc_index = {}\n",
        "with open('doc_index.txt', 'r',encoding='utf-8') as file:\n",
        "\n",
        "    for line in file:\n",
        "        line=line.strip().split()\n",
        "\n",
        "        #line[0] is the document id\n",
        "        #if the document id isnot present in the dictionary then add it\n",
        "        if line[0] not in doc_index:\n",
        "            doc_index[line[0]] = len(line)-2\n",
        "        else:\n",
        "            doc_index[line[0]]+= len(line)-2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwzkeJ8AuDmv",
        "outputId": "8dc26fb2-8c87-4098-b2e0-c854f9ff7208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-6a04628aa24d>:11: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
            "\n",
            "\n",
            "  stop_words_set = set(pd.read_csv('/content/Urdu stopwords.txt', squeeze=True, header=None))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "docids = {}\n",
        "\n",
        "#open the docids.txt file\n",
        "with open(\"docids.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        docid, path = line.strip().split('/')\n",
        "        docids[int(docid)] = path\n",
        "\n",
        "# Load the Word document\n",
        "document = Document('queries.docx')\n",
        "stop_words_set = set(pd.read_csv('/content/Urdu stopwords.txt', squeeze=True, header=None))\n",
        "\n",
        "\n",
        "# Get the first table in the document\n",
        "table = document.tables[0]\n",
        "\n",
        "\n",
        "def remove_non_urdu(text):\n",
        "    urdu_alphabets = u'\\u0600-\\u06FF'\n",
        "\n",
        "    # Remove all the non urdu alphabets and maintain the spaces\n",
        "    text = re.sub('[^%s]' % urdu_alphabets, ' ', text)\n",
        "\n",
        "    # Remove , : ; . from the text\n",
        "    text = re.sub('[,.;:]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Create an empty dictionary to store the data\n",
        "data_dict = {}\n",
        "\n",
        "# Initialize the keys variable to None\n",
        "keys = None\n",
        "\n",
        "# Iterate through each row in the table\n",
        "for i, row in enumerate(table.rows):\n",
        "    \n",
        "    # Get the text content of each cell in the row\n",
        "    text = [cell.text.strip() for cell in row.cells]\n",
        "    \n",
        "    # If this is the first row, save the values as the keys for the dictionary\n",
        "    if i == 0:\n",
        "        keys = tuple(text)\n",
        "        continue\n",
        "    \n",
        "    # If this is not the first row, create a new dictionary with the values in the row\n",
        "    key = text[0]\n",
        "    #print(key, 'j')\n",
        "\n",
        "    values = list(zip(keys[1:], text[1:]))\n",
        "\n",
        "    topics=text[2]\n",
        "    unwanat=text[3]\n",
        "\n",
        "    unwanat=remove_non_urdu(unwanat)\n",
        "    u=unwanat.split()\n",
        "\n",
        "    stemmer = ISRIStemmer()\n",
        "    term_id_dict = {}\n",
        "    \n",
        "    #remove stop words\n",
        "    for i in u:\n",
        "\n",
        "        if i in stop_words_set:\n",
        "            u.remove(i)\n",
        "    # print(u)\n",
        "    #after removing stop words\n",
        "    values.append(u)\n",
        "\n",
        "    # Add the new dictionary to the data dictionary using the key from the first cell as the key\n",
        "    data_dict[key] = values\n",
        "\n",
        "term_id_dict={}\n",
        "\n",
        "# Print the resulting dictionary\n",
        "# print(data_dict)\n",
        "\n",
        "for i in data_dict:\n",
        "\n",
        "    # query\n",
        "    query=data_dict[i][3] \n",
        "\n",
        "    with open('termids.txt', 'r') as f:\n",
        "        for j in f:\n",
        "            term, term_id= j.strip().split('/')\n",
        "            \n",
        "            for q in query:\n",
        "              \n",
        "                if q == term:\n",
        "                    term_id_dict[term]=term_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ck-qwGCuDmz"
      },
      "source": [
        "Function no 1: TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a855_eVsuDm0"
      },
      "outputs": [],
      "source": [
        "\n",
        "tf_idf_for_query={}\n",
        "term_frequency_for_query=0\n",
        "query_id=1\n",
        "\n",
        "for i in data_dict:\n",
        "\n",
        "    # query\n",
        "    query=data_dict[i][3] \n",
        "\n",
        "    for j in query:\n",
        "\n",
        "        if len(j)==0:\n",
        "            continue\n",
        "\n",
        "        if j== 'دی':\n",
        "            continue\n",
        "\n",
        "        #get the term id from the dictionary we have created\n",
        "        term_id=int(term_id_dict[j])\n",
        "\n",
        "        term_frequency_for_query+=query.count(j)\n",
        "        \n",
        "        df=term_info_dict[term_id][2]\n",
        "\n",
        "        #if the freq of the gievn word is zero\n",
        "        if term_frequency_for_query==0:\n",
        "            term_frequency_for_query=1\n",
        "        else:\n",
        "            term_frequency_for_query=1+np.log10(term_frequency_for_query)\n",
        "\n",
        "        #no of documnets\n",
        "        n=5771\n",
        "\n",
        "        #idf for the given term\n",
        "        idf=1+np.log10(n/df)\n",
        "\n",
        "        #get the tf idf for the query\n",
        "        tf_idf_for_query[query_id, term_id]=term_frequency_for_query*idf\n",
        "\n",
        "    query_id+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nx4aTDDluDm2"
      },
      "outputs": [],
      "source": [
        "#dictionary and variable for the tf\n",
        "document_tf={}\n",
        "term_frequency = 0\n",
        "\n",
        "#in the data dictionary we have the data of the queries\n",
        "for key in data_dict:\n",
        "\n",
        "    term_frequency=0\n",
        "\n",
        "    #extract the query from the data dictioanry which has splitted query\n",
        "    query = data_dict[key][3]\n",
        "\n",
        "    for term in query:\n",
        "\n",
        "        term_frequency=0\n",
        "\n",
        "\n",
        "        if term == 'دی':\n",
        "            continue\n",
        "\n",
        "        #if the term length is zero then we have to avoid\n",
        "        if len(term) == 0:\n",
        "            continue\n",
        "\n",
        "        #find term id of the term\n",
        "        termid = term_id_dict[term]\n",
        "\n",
        "        #iterate through the list of document ids\n",
        "        for docid in docids:\n",
        "            #for every documnent count the number of times the term appears\n",
        "            term_frequency = len(inverted_index[int(termid)][docid])\n",
        "            \n",
        "            #as trhe term frequency of zero should be one after normalization\n",
        "            if term_frequency == 0:\n",
        "\n",
        "                #term frequncy = 1\n",
        "                term_frequency = 1\n",
        "            else:\n",
        "\n",
        "                #normalize the term frequency \n",
        "                term_frequency = 1 + np.log10(term_frequency)\n",
        "\n",
        "            #store the query # term # and tf-idf score in the dictionary\n",
        "            document_tf[docid,termid]=term_frequency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "X0gFuzBluDm4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# create a dictionary to store the cosine similarity between each query and document\n",
        "cosine_similarity_dictionary = {}\n",
        "\n",
        "# calculate cosine similarity between each query and document\n",
        "for query_key, query_tf_idf in tf_idf_for_query.items():\n",
        "\n",
        "    #we have the query_id and term_id for the elements of the query\n",
        "    query_id, _ = query_key\n",
        "\n",
        "    #create a list at every query_id\n",
        "    cosine_similarity_dictionary[query_id] = defaultdict(int)\n",
        "  \n",
        "    for doc_key, doc_tf in document_tf.items():\n",
        "    \n",
        "        #document_id and term id for the doumnent \n",
        "        doc_id, _ = doc_key\n",
        "\n",
        "        #dot product\n",
        "        dot_product = 0.0\n",
        "      \n",
        "        # calculate dot product and magnitude of query vector\n",
        "        dot_product += query_tf_idf * doc_tf\n",
        "    \n",
        "        #if query_id and doc_id already exists\n",
        "        if query_id in cosine_similarity_dictionary and doc_id in cosine_similarity_dictionary[query_id]:\n",
        "            cosine_similarity_dictionary[query_id][doc_id] = dot_product \n",
        "        else:\n",
        "\n",
        "            #add the dot product to query\n",
        "            cosine_similarity_dictionary[query_id][doc_id] += dot_product \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eHL8TOiuDm5"
      },
      "source": [
        "Function no 2 : OKAPA BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j2WpBgoZuDm6"
      },
      "outputs": [],
      "source": [
        "#find the average document length\n",
        "average_length = 0\n",
        "for key in doc_index:\n",
        "    average_length += doc_index[key]\n",
        "\n",
        "\n",
        "no_of_docs=5771\n",
        "avg_doc_len=average_length/no_of_docs\n",
        "\n",
        "#calculate the okapi bm25 score\n",
        "k=1.2\n",
        "b=0.75\n",
        "okapi_bm25_dic = {}\n",
        "\n",
        "query_id=1\n",
        "for key in data_dict:\n",
        "        \n",
        "        #create the okapi dictionary for every query\n",
        "        okapi_bm25_dic[query_id] = defaultdict(int)\n",
        "\n",
        "        term_frequency=0\n",
        "\n",
        "        #get the query\n",
        "        query = data_dict[key][3]\n",
        "    \n",
        "        for term in query:\n",
        "\n",
        "            term_frequency=0\n",
        "\n",
        "            if term == 'دی':\n",
        "                continue\n",
        "            if len(term) == 0:\n",
        "                continue\n",
        "\n",
        "\n",
        "            #find term id of the term\n",
        "            term_id = term_id_dict[term]\n",
        "\n",
        "            #get the document frequency of the term\n",
        "            document_freq = term_info_dict[int(term_id)][2]\n",
        "\n",
        "            #iterate through the list of document ids\n",
        "            for docid in docids:\n",
        "                \n",
        "                #for every document get the tf of the term with the document\n",
        "                term_frequency=document_tf[docid,term_id]\n",
        "\n",
        "                #get the term frequency for the query\n",
        "                term_freq_for_query = query.count(term)\n",
        "\n",
        "                if term_freq_for_query==0:\n",
        "                    term_freq_for_query=1\n",
        "                else:\n",
        "\n",
        "                    #term frequncy for the query\n",
        "                    term_freq_for_query = 1 + np.log10(term_freq_for_query)\n",
        "\n",
        "                a=(k*(1-b+b*doc_index[str(docid)]/avg_doc_len)+term_frequency)  *  (k+1)*term_freq_for_query / (k+term_freq_for_query)\n",
        "                \n",
        "                #calculte the okapi bm 25 using the formula already given \n",
        "                okapi_bm25=  np.log10(5771+0.5 /  document_freq+0.5)  *  (k+1)*term_frequency / a\n",
        "\n",
        "                #if the queryid and the document id are not already avaliable\n",
        "                if query_id and docid not in okapi_bm25_dic:\n",
        "                    okapi_bm25_dic[query_id][docid] = okapi_bm25\n",
        "\n",
        "                else:\n",
        "\n",
        "                    #if it is already present then add to the previous values\n",
        "                    okapi_bm25_dic[query_id][docid] += okapi_bm25\n",
        "            \n",
        "        query_id+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRzxB3dJuDm8"
      },
      "source": [
        "FUCNTION NO 3:Dirichlet Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MTjk8-CvuDm9"
      },
      "outputs": [],
      "source": [
        "#calculate the total number of terms in the corpus\n",
        "total_terms = 0\n",
        "\n",
        "#iterate in the doc index and calculate the total terms\n",
        "for i in doc_index:\n",
        "    total_terms += doc_index[i]\n",
        "\n",
        "#calculate the language model with dirichlet smoothing\n",
        "dirichlet_smoothing_dic = {}\n",
        "query_id=1\n",
        "\n",
        "for key in data_dict:\n",
        "        \n",
        "\n",
        "        dirichlet_smoothing_dic[query_id] = defaultdict(int)\n",
        "        query = data_dict[key][3]\n",
        "        \n",
        "        #for every term in the query\n",
        "        for term in query:\n",
        "            if term == 'دی':\n",
        "                continue\n",
        "\n",
        "            if len(term) == 0:\n",
        "                continue\n",
        "\n",
        "            #find term id of the term\n",
        "            term_id = term_id_dict[term]\n",
        "\n",
        "            #get the document frequency of the term\n",
        "            df = term_info_dict[int(term_id)][2]\n",
        "\n",
        "            #A is the average length\n",
        "            A=average_length\n",
        "            \n",
        "            #get the total frequency of the term in the corpus\n",
        "            total_tf = term_info_dict[int(term_id)][1]\n",
        "\n",
        "            #iterate through the list of document ids\n",
        "            for docid in docids:\n",
        "                #for every document get the tf of the term with the document\n",
        "                tf=document_tf[docid,term_id]\n",
        "\n",
        "                #get the length of this document\n",
        "                N = doc_index[str(docid)]\n",
        "\n",
        "                #calculate the language model with dirichlet smoothing\n",
        "                dirichlet_smoothing = np.log10((N / (N + A)) * (tf / N))  + ((A / (N + A)) * (total_tf/(A*5771)))\n",
        "\n",
        "                if query_id and docid not in dirichlet_smoothing_dic:\n",
        "                    dirichlet_smoothing_dic[query_id][docid] = dirichlet_smoothing\n",
        "\n",
        "                else:\n",
        "\n",
        "                    #if it is already available then add\n",
        "                    dirichlet_smoothing_dic[query_id][docid] += dirichlet_smoothing\n",
        "        query_id+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYBv-vKXuDm_"
      },
      "source": [
        "Sorting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GzJ85vhPuDnA"
      },
      "outputs": [],
      "source": [
        "sorted_cosine = defaultdict(list)\n",
        "\n",
        "for query_id in data_dict:\n",
        "  \n",
        "  #get the query id and convert it into integer\n",
        "  query_id=int(query_id)\n",
        "\n",
        "  #for every document present in the document ids\n",
        "  for docid in docids:\n",
        "\n",
        "    sorted_cosine[query_id].append(f\"{docid}:{cosine_similarity_dictionary[query_id][docid]}\")\n",
        "\n",
        "\n",
        "#iterate\n",
        "for i in sorted_cosine:\n",
        "\n",
        "  #sort \n",
        "  sorted_cosine[i] = sorted(sorted_cosine[i], key=lambda i: float(i.split(':')[1]), reverse=True)\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZdLYT-i9uDnB"
      },
      "outputs": [],
      "source": [
        "sorted_okapi = defaultdict(list)\n",
        "\n",
        "for query_id in data_dict:\n",
        "  \n",
        "  #get the query id and convert it into integer\n",
        "  query_id=int(query_id)\n",
        "\n",
        "  #for every document present in the document ids\n",
        "  for docid in docids:\n",
        "\n",
        "    sorted_okapi[query_id].append(f\"{docid}:{okapi_bm25_dic[query_id][docid]}\")\n",
        "\n",
        "\n",
        "#iterate\n",
        "for i in sorted_cosine:\n",
        "\n",
        "  #sort \n",
        "  sorted_okapi[i] = sorted(sorted_cosine[i], key=lambda i: float(i.split(':')[1]), reverse=True)\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RdAt4--guDnC"
      },
      "outputs": [],
      "source": [
        "\n",
        "#sorting the dirichlet_smoothing_dic\n",
        "\n",
        "#first add\n",
        "sorted_dirch = defaultdict(list)\n",
        "for query_id in data_dict:\n",
        "  \n",
        "  #get the query id and convert it into integer\n",
        "  query_id=int(query_id)\n",
        "\n",
        "  #for every document present in the document ids\n",
        "  for docid in docids:\n",
        "    sorted_dirch[query_id].append(f\"{docid}:{dirichlet_smoothing_dic[query_id][docid]}\")\n",
        "\n",
        "#iterate and sort\n",
        "for i in sorted_dirch:\n",
        "  sorted_dirch[i] = sorted(sorted_dirch[i], key=lambda i: float(i.split(':')[1]), reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_-r63SWuDnD"
      },
      "source": [
        "QRELS FILE "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BJOCQHrHuDnE"
      },
      "outputs": [],
      "source": [
        "doc_ids = {}\n",
        "with open(\"docids.txt\", 'r') as file:\n",
        "  for line in file:\n",
        "    value, key = line.strip().split('/')\n",
        "    doc_ids[int(key.split('.')[0])] = int(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bI08jq33uDnF"
      },
      "outputs": [],
      "source": [
        "file = pd.ExcelFile(\"qrels.xlsx\")\n",
        "\n",
        "#get the names\n",
        "sheet_names = file.sheet_names\n",
        "\n",
        "#qrels dictionary\n",
        "qrels = {}\n",
        "\n",
        "#iterate in the list of names\n",
        "for i in sheet_names:\n",
        "    qrels[i] = file.parse(i)\n",
        "\n",
        "\n",
        "qrel = {}\n",
        "\n",
        "for i, query_id in zip(qrels.keys(), data_dict):\n",
        "\n",
        "  qrel[query_id] = defaultdict(int)\n",
        "\n",
        "  #iaterate and get the document id\n",
        "  for docid in [int(x) for x in qrels[i]['Doc Id'] if int(x) in doc_ids.keys()]:\n",
        "\n",
        "    #add to the qrel dictionayr\n",
        "    qrel[query_id][doc_ids[docid]] = int(qrels[i].loc[qrels[i]['Topic Id'] == int(query_id)].loc[qrels[i]['Doc Id'] == int(docid) ]['Doc Relevancy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IjWQ2AImuDnG"
      },
      "outputs": [],
      "source": [
        "#evaluate the model\n",
        "def evaluate(hello):\n",
        "\n",
        "  #precision at 5\n",
        "  prec_at_5 = defaultdict(list)\n",
        "\n",
        "  #precision at 10\n",
        "  prec_at_10 = defaultdict(list) \n",
        "\n",
        "  #precision at 20\n",
        "  prec_at_20 = defaultdict(list)\n",
        "\n",
        "  #precision at 30\n",
        "  prec_at_30 = defaultdict(list)\n",
        "\n",
        "  #iterate through the queries\n",
        "  for q in data_dict:\n",
        "\n",
        "    #precision at 5\n",
        "    prec_at_5[int(q)] = [int(hello[int(q)][i].strip().split(':')[0]) for i in range(5)]\n",
        "\n",
        "    #precision at 10\n",
        "    prec_at_10[int(q)] = [int(hello[int(q)][i].strip().split(':')[0]) for i in range(10)]\n",
        "\n",
        "    #precision at 20\n",
        "    prec_at_20[int(q)] = [int(hello[int(q)][i].strip().split(':')[0]) for i in range(20)]\n",
        "\n",
        "    #precision at 30\n",
        "    prec_at_30[int(q)] = [int(hello[int(q)][i].strip().split(':')[0]) for i in range(30)]\n",
        "\n",
        "  #return the precision at 5, 10, 20, 30\n",
        "  return [prec_at_5, prec_at_10, prec_at_20, prec_at_30]\n",
        "\n",
        "#evaluate the cosine similarity model\n",
        "evaluation_cosine = evaluate(sorted_cosine)\n",
        "\n",
        "#evaluate the okapi bm25 model\n",
        "evaluation_okapi = evaluate(sorted_okapi)\n",
        "\n",
        "#evaluate the language model with dirichlet smoothing\n",
        "evaltion_dirch = evaluate(sorted_dirch)\n",
        "\n",
        "\n",
        "final = [evaluation_cosine,evaluation_okapi,evaltion_dirch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rpHTFs6yuDnH"
      },
      "outputs": [],
      "source": [
        "# iterate over the final list and calculate the precision at k\n",
        "for i, evalation in enumerate(final):\n",
        "\n",
        "  for j, p in enumerate(evalation):\n",
        "\n",
        "    # iterate over the 10 queries\n",
        "    for k, queryid in enumerate(data_dict):\n",
        "\n",
        "      #relevance value\n",
        "      relevance = 0\n",
        "      non_relevance = 0\n",
        "\n",
        "      for docids in p[int(queryid)]:\n",
        "        if qrel[queryid][docids] > 2:\n",
        "          relevance += 1\n",
        "        else:\n",
        "          non_relevance += 1\n",
        "\n",
        "      temp=relevance/(relevance+non_relevance)\n",
        "      # calculate the precision at \n",
        "      final[i][j][k+1].append(temp)\n",
        "\n",
        "average_precision = {}\n",
        "\n",
        "#map function\n",
        "def map(score, id):\n",
        "\n",
        "  #avd_p is a list\n",
        "  average_temp = []\n",
        "\n",
        "  #query number\n",
        "  for query_id in data_dict:\n",
        "\n",
        "    #make a dictionary for each query\n",
        "    average_precision[id] = defaultdict(int)\n",
        "\n",
        "    #make a dictionary for each query\n",
        "    average_precision[id][int(query_id)] = defaultdict(int)\n",
        "\n",
        "    #relevance value\n",
        "    relevance = 0\n",
        "\n",
        "    #sum\n",
        "    sumd = 0\n",
        "\n",
        "    #for each document in the query\n",
        "    for ind, vals in enumerate(score[int(query_id)]):\n",
        "\n",
        "      #get the document id\n",
        "      docid = int(vals.strip().split(':')[0])\n",
        "\n",
        "      #if the document is relevant\n",
        "      if qrel[query_id][docid] > 2:\n",
        "\n",
        "        #increment the relevance value\n",
        "        relevance += 1\n",
        "\n",
        "        #calculate the average precision\n",
        "        average_precision[id][int(query_id)][ind] = relevance/(ind+1)\n",
        "\n",
        "        #add the average precision to the sum\n",
        "        sumd += average_precision[id][int(query_id)][ind]\n",
        "\n",
        "    #append the average precision to the list\n",
        "    b=sumd/relevance\n",
        "    average_temp.append(b)\n",
        "\n",
        "  a = sum(average_temp)/len(data)\n",
        "\n",
        "  #return the mean average precision\n",
        "  return a\n",
        "\n",
        "#calculate the mean average precision for each model\n",
        "map1 = map(sorted_cosine, 0)\n",
        "\n",
        "#calculate the mean average precision for each model\n",
        "map2 = map(sorted_dirch, 1)\n",
        "\n",
        "#calculate the mean average precision for each model\n",
        "map3 = map(sorted_dirch, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KSdzLUthuDnJ"
      },
      "outputs": [],
      "source": [
        "with open(\"tfidf.txt\", 'w') as file:\n",
        "  for q in data_dict:\n",
        "    for vals in sorted_cosine[int(q) ]:\n",
        "      file.write(f\"{q} {vals}\\n\")\n",
        "\n",
        "\n",
        "with open(\"okapi bm25.txt\", 'w') as file:\n",
        "  for q in data_dict:\n",
        "    for vals in sorted_okapi[int(q) ]:\n",
        "      file.write(f\"{q} {vals}\\n\")\n",
        "\n",
        "\n",
        "with open(\"Dirichlet Smoothing.txt\", 'w') as file:\n",
        "  for q in data_dict:\n",
        "    for vals in sorted_dirch[int(q) ]:\n",
        "      file.write(f\"{q} {vals}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gEz7Y_xuDnK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}